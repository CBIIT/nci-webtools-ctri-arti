openapi: 3.1.0
info:
  title: Gateway Service API
  description: |
    Internal AI inference service. Provides a unified interface for running model
    inference across multiple providers (AWS Bedrock, Google Gemini). Handles rate
    limiting and usage tracking.
  version: 1.0.0

servers:
  - url: http://localhost:3001
    description: Local development

paths:
  /api/infer:
    post:
      operationId: infer
      summary: Run AI model inference
      description: |
        Main inference endpoint. Supports streaming and non-streaming responses.
        If `userId` is provided, checks rate limits before inference and tracks
        usage after completion.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/InferRequest"
      responses:
        "200":
          description: Inference result (non-streaming)
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/InferResponse"
            application/x-ndjson:
              schema:
                $ref: "#/components/schemas/StreamChunk"
              description: |
                Streaming response â€” newline-delimited JSON. Each line is a
                separate message object. The final message contains
                `metadata.usage` for token tracking.
        "429":
          description: Rate limit exceeded
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
              example:
                error: "You have reached your allocated weekly usage limit. Your access to the chat tool is temporarily disabled and will reset on Monday at 12:00 AM."
        "500":
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /api/models:
    get:
      operationId: listModels
      summary: List available models
      description: Returns all models with `providerId: 1` (Bedrock).
      responses:
        "200":
          description: List of available models
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: "#/components/schemas/ModelInfo"

  /health:
    get:
      operationId: healthCheck
      summary: Health check
      responses:
        "200":
          description: Service is healthy
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                    example: ok

components:
  schemas:
    InferRequest:
      type: object
      required:
        - model
        - messages
      properties:
        userId:
          type: integer
          nullable: true
          description: User ID for rate limiting and usage tracking. Optional.
        model:
          type: string
          description: Model internal name (e.g., `us.anthropic.claude-opus-4-6-v1`).
        messages:
          type: array
          items:
            $ref: "#/components/schemas/Message"
          description: Conversation messages.
        system:
          type: string
          description: System prompt text.
        tools:
          type: array
          items:
            type: object
            properties:
              toolSpec:
                type: object
          description: Tool specifications for function calling.
        thoughtBudget:
          type: integer
          default: 0
          description: Token budget for extended thinking. Set to 0 to disable.
        stream:
          type: boolean
          default: false
          description: Whether to stream the response.
        ip:
          type: string
          description: Client IP address for usage tracking.
        outputConfig:
          type: object
          description: Additional output configuration passed to the provider.

    InferResponse:
      type: object
      properties:
        content:
          type: array
          items:
            type: object
          description: Response content blocks (text, tool use, etc.).
        usage:
          $ref: "#/components/schemas/UsageData"
        stopReason:
          type: string
          enum: [end_turn, tool_use, max_tokens, stop_sequence]

    StreamChunk:
      type: object
      description: |
        A single chunk in a streaming response. Most chunks contain content
        deltas. The final chunk contains `metadata.usage`.
      properties:
        contentBlockIndex:
          type: integer
        contentBlockStart:
          type: object
        contentBlockDelta:
          type: object
        contentBlockStop:
          type: object
        metadata:
          type: object
          properties:
            usage:
              $ref: "#/components/schemas/UsageData"

    Message:
      type: object
      required:
        - role
        - content
      properties:
        role:
          type: string
          enum: [user, assistant]
        content:
          type: array
          items:
            type: object
            properties:
              text:
                type: string
              type:
                type: string

    ModelInfo:
      type: object
      properties:
        name:
          type: string
          description: Display name (e.g., "Opus 4.6").
        internalName:
          type: string
          description: Provider model ID (e.g., "us.anthropic.claude-opus-4-6-v1").
        maxContext:
          type: integer
          description: Maximum context window in tokens.
        maxOutput:
          type: integer
          description: Maximum output tokens.
        maxReasoning:
          type: integer
          description: Maximum reasoning/thinking tokens.

    UsageData:
      type: object
      properties:
        inputTokens:
          type: number
        outputTokens:
          type: number
        cacheReadInputTokens:
          type: number
        cacheWriteInputTokens:
          type: number

    Error:
      type: object
      properties:
        error:
          type: string
